import os
import pandas as pd
from openai import OpenAI
import random
import json
import time
import re
from pathlib import Path
from tqdm import tqdm
import logging

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# ë°©ë²• 1: ì§ì ‘ ì…ë ¥ (ì•„ë˜ YOUR_API_KEY ë¶€ë¶„ì— ì‹¤ì œ í‚¤ ì…ë ¥)
# client = OpenAI(api_key="YOUR_API_KEY")

# ë°©ë²• 2: í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© (ë³´ì•ˆìƒ ê¶Œì¥)
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì…ë ¥ëœ í‚¤ ì‚¬ìš©
    api_key = "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"  # ì—¬ê¸°ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
    if api_key == "YOUR_API_KEY":
        raise ValueError("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì½”ë“œ 17ë²ˆì§¸ ì¤„ì—ì„œ 'YOUR_API_KEY'ë¥¼ ì‹¤ì œ í‚¤ë¡œ ë³€ê²½í•˜ì„¸ìš”.")

client = OpenAI(api_key=api_key)

def load_excel_data(file_path="dictionary_data.xlsx"):
    """Excel íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤."""
    if not Path(file_path).exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
        required_sheets = ['ìœ„ì¹˜', 'ì„¤ë¹„ìœ í˜•', 'í˜„ìƒì½”ë“œ', 'ìš°ì„ ìˆœìœ„']
        
        for sheet in required_sheets:
            if sheet not in xls.sheet_names:
                raise ValueError(f"í•„ìˆ˜ ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {sheet}")
        
        df_location = pd.read_excel(xls, sheet_name='ìœ„ì¹˜')
        df_equipment = pd.read_excel(xls, sheet_name='ì„¤ë¹„ìœ í˜•')
        df_phenomenon = pd.read_excel(xls, sheet_name='í˜„ìƒì½”ë“œ')
        df_priority = pd.read_excel(xls, sheet_name='ìš°ì„ ìˆœìœ„')
        
        location_list = df_location.iloc[:, 0].dropna().tolist()
        equipment_list = df_equipment.iloc[:, 0].dropna().tolist()
        phenomenon_list = df_phenomenon.iloc[:, 0].dropna().tolist()
        priority_list = df_priority.iloc[:, 0].dropna().tolist()
        
        # í•œêµ­ì–´ í¬í•¨ ì„¤ë¹„ìœ í˜• ë³„ë„ ì¶”ì¶œ
        korean_equipment_list = []
        for eq in equipment_list:
            has_korean = any('\uac00' <= char <= '\ud7a3' for char in str(eq))
            if has_korean:
                korean_equipment_list.append(eq)
        
        logger.info(f"í•œêµ­ì–´ í¬í•¨ ì„¤ë¹„ìœ í˜•: {len(korean_equipment_list)}ê°œ")
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        if not all([location_list, equipment_list, phenomenon_list, priority_list]):
            raise ValueError("í•˜ë‚˜ ì´ìƒì˜ ì‹œíŠ¸ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ìœ„ì¹˜: {len(location_list)}, ì„¤ë¹„: {len(equipment_list)}, "
                   f"í˜„ìƒ: {len(phenomenon_list)}, ìš°ì„ ìˆœìœ„: {len(priority_list)}")
        
        return location_list, equipment_list, phenomenon_list, priority_list, korean_equipment_list
        
    except Exception as e:
        logger.error(f"Excel íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

# âœ… ì‚¬ì „ ë°ì´í„° ë¡œë“œ
location_list, equipment_list, phenomenon_list, priority_list, korean_equipment_list = load_excel_data()

def generate_combinations(target_num=10000, missing_ratio=0.15):
    """ì§€ì •ëœ ìˆ˜ë§Œí¼ ê³ ìœ í•œ ì¡°í•©ì„ ìƒì„±í•©ë‹ˆë‹¤. ì¼ë¶€ í•„ë“œëŠ” 'ë¯¸ì…ë ¥'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
    logger.info(f"ì´ {target_num}ê°œì˜ ì¡°í•© ìƒì„± ì‹œì‘ (ë¯¸ì…ë ¥ ë¹„ìœ¨: {missing_ratio*100:.1f}%)...")
    
    # ê° í•„ë“œì— 'ë¯¸ì…ë ¥' ì¶”ê°€
    extended_location_list = location_list + ['ë¯¸ì…ë ¥']
    extended_equipment_list = equipment_list + ['ë¯¸ì…ë ¥']
    extended_phenomenon_list = phenomenon_list + ['ë¯¸ì…ë ¥']
    extended_priority_list = priority_list + ['ë¯¸ì…ë ¥']
    
    # ìµœëŒ€ ê°€ëŠ¥í•œ ì¡°í•© ìˆ˜ ê³„ì‚°
    max_combinations = len(extended_location_list) * len(extended_equipment_list) * len(extended_phenomenon_list) * len(extended_priority_list)
    if target_num > max_combinations:
        logger.warning(f"ìš”ì²­í•œ ì¡°í•© ìˆ˜({target_num})ê°€ ìµœëŒ€ ê°€ëŠ¥ ì¡°í•© ìˆ˜({max_combinations})ë³´ë‹¤ í½ë‹ˆë‹¤. "
                      f"ìµœëŒ€ ê°€ëŠ¥ ìˆ˜ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
        target_num = max_combinations
    
    selected_combos = set()
    attempts = 0
    max_attempts = target_num * 10  # ë¬´í•œ ë£¨í”„ ë°©ì§€
    
    while len(selected_combos) < target_num and attempts < max_attempts:
        # ê° í•„ë“œë³„ë¡œ ë¯¸ì…ë ¥ ì—¬ë¶€ ê²°ì •
        use_missing_location = random.random() < missing_ratio
        use_missing_equipment = random.random() < missing_ratio
        use_missing_phenomenon = random.random() < missing_ratio
        use_missing_priority = random.random() < missing_ratio
        
        # ì ì–´ë„ í•˜ë‚˜ì˜ í•„ë“œëŠ” ì…ë ¥ë˜ì–´ì•¼ í•¨
        if all([use_missing_location, use_missing_equipment, use_missing_phenomenon, use_missing_priority]):
            # ëœë¤í•˜ê²Œ í•˜ë‚˜ì˜ í•„ë“œëŠ” ì…ë ¥ë˜ë„ë¡ í•¨
            field_to_keep = random.choice(['location', 'equipment', 'phenomenon', 'priority'])
            if field_to_keep == 'location':
                use_missing_location = False
            elif field_to_keep == 'equipment':
                use_missing_equipment = False
            elif field_to_keep == 'phenomenon':
                use_missing_phenomenon = False
            else:
                use_missing_priority = False
        
        combo = (
            'ë¯¸ì…ë ¥' if use_missing_location else random.choice(location_list),
            'ë¯¸ì…ë ¥' if use_missing_equipment else random.choice(equipment_list),
            'ë¯¸ì…ë ¥' if use_missing_phenomenon else random.choice(phenomenon_list),
            'ë¯¸ì…ë ¥' if use_missing_priority else random.choice(priority_list)
        )
        selected_combos.add(combo)
        attempts += 1
    
    selected_combos = list(selected_combos)
    random.shuffle(selected_combos)
    
    # ë¯¸ì…ë ¥ í†µê³„
    missing_counts = {'ìœ„ì¹˜': 0, 'ì„¤ë¹„ìœ í˜•': 0, 'í˜„ìƒì½”ë“œ': 0, 'ìš°ì„ ìˆœìœ„': 0}
    for combo in selected_combos:
        if combo[0] == 'ë¯¸ì…ë ¥':
            missing_counts['ìœ„ì¹˜'] += 1
        if combo[1] == 'ë¯¸ì…ë ¥':
            missing_counts['ì„¤ë¹„ìœ í˜•'] += 1
        if combo[2] == 'ë¯¸ì…ë ¥':
            missing_counts['í˜„ìƒì½”ë“œ'] += 1
        if combo[3] == 'ë¯¸ì…ë ¥':
            missing_counts['ìš°ì„ ìˆœìœ„'] += 1
    
    logger.info(f"ì¡°í•© ìƒì„± ì™„ë£Œ: {len(selected_combos)}ê°œ")
    logger.info(f"ë¯¸ì…ë ¥ í†µê³„ - ìœ„ì¹˜: {missing_counts['ìœ„ì¹˜']}, ì„¤ë¹„ìœ í˜•: {missing_counts['ì„¤ë¹„ìœ í˜•']}, "
               f"í˜„ìƒì½”ë“œ: {missing_counts['í˜„ìƒì½”ë“œ']}, ìš°ì„ ìˆœìœ„: {missing_counts['ìš°ì„ ìˆœìœ„']}")
    
    return selected_combos

# âœ… ì¡°í•© ìˆ˜ ì„¤ì • (10,000ê°œë¡œ ë³€ê²½)
target_num = 10000  # ë³¸ê²© ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ
missing_field_ratio = 0.15  # ê° í•„ë“œë³„ ë¯¸ì…ë ¥ ë¹„ìœ¨ (15%)
selected_combos = generate_combinations(target_num, missing_field_ratio)

# âœ… GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë‹¤ì–‘í•œ ì˜¤ë¥˜ íŒ¨í„´ í¬í•¨)
system_prompt = (
    "You are a maintenance worker writing equipment fault reports. Create natural Korean sentences with common errors based on the given location, equipment type, fault code, and priority information.\n\n"
    "IMPORTANT: Some fields may be marked as 'ë¯¸ì…ë ¥' (not entered). In these cases, create sentences that naturally omit or vaguely reference those missing fields.\n\n"
    "Include these error types:\n"
    "1. Korean-English mixing (í•œì˜ í˜¼ìš©)\n"
    "2. English to Korean translation (ì˜ì–´â†’í•œêµ­ì–´ ë²ˆì—­: Pumpâ†’íŒí”„, Valveâ†’ë°¸ë¸Œ)\n"
    "3. English to Korean phonetic transcription (ì˜ì–´â†’í•œêµ­ì–´ ìŒì°¨: Pressureâ†’í”„ë ˆì…”, Vesselâ†’ë² ì ¤)\n"
    "4. Spacing errors (ë„ì–´ì“°ê¸° ì˜¤ë¥˜)\n" 
    "5. Typos and phonetic variations (ì˜¤íƒ€ ë° ìŒì„±í•™ì  ë³€í˜•)\n"
    "6. Punctuation inconsistencies (êµ¬ë‘ì  ë¶ˆì¼ì¹˜)\n"
    "7. Missing field handling (ë¯¸ì…ë ¥ í•„ë“œ ìì—°ìŠ¤ëŸ¬ìš´ ì²˜ë¦¬)\n\n"
    
    "Examples:\n\n"
    
    "ì˜ˆì‹œ1 - í•œì˜í˜¼ìš© + ìŒì„±í•™ì  ë³€í˜•:\n"
    "ìœ„ì¹˜: No.1 PE, ì„¤ë¹„ìœ í˜•: Pressure Vessel/Drum, í˜„ìƒì½”ë“œ: ê³ ì¥, ìš°ì„ ìˆœìœ„: ê¸´ê¸‰\n"
    "ë¬¸ì¥: NO 1 peì˜ í”„ë ˆì…” ë² ì ¤ ë“œëŸ¼ì— ê³ ì¥ ë°œìƒ. ê¸´ê¸‰ì¡°ì¹˜ ìš”ë§.\n\n"
    
    "ì˜ˆì‹œ2 - ì˜¤íƒ€ + êµ¬ë¶„ì ë³€í˜•:\n"
    "ìœ„ì¹˜: ì„ìœ ì œí’ˆë°°í•©/ì €ì¥, ì„¤ë¹„ìœ í˜•: Motor Operated Valve, í˜„ìƒì½”ë“œ: ì‘ë™ë¶ˆëŸ‰, ìš°ì„ ìˆœìœ„: ìš°ì„ \n"
    "ë¬¸ì¥: ì„œê·œ ì œí’ˆë°°í•©-ì €ì¥, ëª¨í„° ë²¨ë¸Œ, ì˜¤ì‘ë™ ë°œìƒ. ìš°ì„  ì‘ì—… ìš”ì²­.\n\n"
    
    "ì˜ˆì‹œ3 - ë„ì–´ì“°ê¸° ì˜¤ë¥˜ + êµ¬ë‘ì  ëˆ„ë½:\n"
    "ìœ„ì¹˜: í•©ì„±ìˆ˜ì§€ 1ì°½ê³ , ì„¤ë¹„ìœ í˜•: Conveyor Belt, í˜„ìƒì½”ë“œ: íŒŒì†, ìš°ì„ ìˆœìœ„: ë³´í†µ\n"
    "ë¬¸ì¥: í•©ì„±ìˆ˜ì§€, 1ì°½ê³  7ë²ˆ ë¼ì¸ - ì»¨ë² ì´ì–´ belt íŒŒì† í™•ì¸ ë³´í†µ.\n\n"
    
    "ì˜ˆì‹œ4 - ë‹¤ì–‘í•œ êµ¬ë¶„ì + ì•½ì–´:\n"
    "ìœ„ì¹˜: ì¤‘ì•™ì œì–´ì‹¤, ì„¤ë¹„ìœ í˜•: Control Panel, í˜„ìƒì½”ë“œ: ì´ìƒ, ìš°ì„ ìˆœìœ„: ì ê²€\n"
    "ë¬¸ì¥: ì¤‘ì•™ ì œì–´ì‹¤ ì»¨íŠ¸ë¡¤ íŒ¨ë„ ì´ìƒ . ì ê²€ ìŠ¤ì¼€ì¥´ë§.\n\n"
    
    "ì˜ˆì‹œ5 - ì˜ì–´ì˜ í•œê¸€ í‘œê¸° + í˜¼í•© êµ¬ë¶„ì:\n"
    "ìœ„ì¹˜: ì›ë£Œì €ì¥íƒ±í¬, ì„¤ë¹„ìœ í˜•: Storage Tank, í˜„ìƒì½”ë“œ: ëˆ„ì„¤, ìš°ì„ ìˆœìœ„: ê¸´ê¸‰\n"
    "ë¬¸ì¥: ì›ë£Œ ì €ì¥ íƒ±í¬ ìŠ¤í† ë¦¬ì§€ íƒ±í¬/ ëˆ„ì„¤ ì‚¬ê³ . ê¸´ê¸‰ ëŒ€ì‘ìš”ë§!.\n\n"
    
    "ì˜ˆì‹œ8 - ì˜ì–´ ì„¤ë¹„ëª…ì˜ í•œêµ­ì–´ ë²ˆì—­:\n"
    "ìœ„ì¹˜: ì¤‘ì•™ì œì–´ì‹¤, ì„¤ë¹„ìœ í˜•: Heat Exchanger, í˜„ìƒì½”ë“œ: ì˜¨ë„ì´ìƒ, ìš°ì„ ìˆœìœ„: ìš°ì„ \n"
    "ë¬¸ì¥: ì¤‘ì•™ì œì–´ì‹¤ ì—´êµí™˜ê¸° ì˜¨ë„ ì´ìƒ ë°œê²¬. ìš°ì„  ì ê²€ ìš”ì²­.\n\n"
    
    "ì˜ˆì‹œ9 - ì˜ì–´ ì„¤ë¹„ëª…ì˜ ìŒì°¨ + í•œêµ­ì–´ í˜¼ìš©:\n"
    "ìœ„ì¹˜: ë³´ì¼ëŸ¬ì‹¤, ì„¤ë¹„ìœ í˜•: Centrifugal Pump, í˜„ìƒì½”ë“œ: ì§„ë™, ìš°ì„ ìˆœìœ„: ê¸´ê¸‰\n"
    "ë¬¸ì¥: ë³´ì¼ëŸ¬ì‹¤ ì„¼íŠ¸ë¦¬í“¨ê°ˆ íŒí”„ ì§„ë™ ì‹¬í•¨. ê¸´ê¸‰ ìˆ˜ë¦¬ í•„ìš”.\n\n"
    
    "ì˜ˆì‹œ10 - ë³µí•© ë³€í™˜ (ë²ˆì—­+ìŒì°¨+ì•½ì–´):\n"
    "ìœ„ì¹˜: ëƒ‰ê°íƒ‘, ì„¤ë¹„ìœ í˜•: Motor Operated Valve, í˜„ìƒì½”ë“œ: ì‘ë™ë¶ˆëŸ‰, ìš°ì„ ìˆœìœ„: ì ê²€\n"
    "ë¬¸ì¥: ëƒ‰ê°íƒ‘ ëª¨í„° êµ¬ë™ ë°¸ë¸Œ MOV ì‘ë™ ë¶ˆëŸ‰. ì ê²€ ìŠ¤ì¼€ì¤„ë§.\n\n"
    
    "ì˜ˆì‹œ6 - ë¯¸ì…ë ¥ í•„ë“œ ì²˜ë¦¬ (ìœ„ì¹˜ ë¯¸ì…ë ¥):\n"
    "ìœ„ì¹˜: ë¯¸ì…ë ¥, ì„¤ë¹„ìœ í˜•: Pump, í˜„ìƒì½”ë“œ: ì´ìƒìŒ, ìš°ì„ ìˆœìœ„: ì ê²€\n"
    "ë¬¸ì¥: íŒí”„ì—ì„œ ì´ìƒìŒ ë°œìƒ. ì ê²€ í•„ìš”.\n\n"
    
    "ì˜ˆì‹œ7 - ë¯¸ì…ë ¥ í•„ë“œ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ ë¯¸ì…ë ¥):\n"
    "ìœ„ì¹˜: ë³´ì¼ëŸ¬ì‹¤, ì„¤ë¹„ìœ í˜•: Heat Exchanger, í˜„ìƒì½”ë“œ: ì˜¨ë„ì´ìƒ, ìš°ì„ ìˆœìœ„: ë¯¸ì…ë ¥\n"
    "ë¬¸ì¥: ë³´ì¼ëŸ¬ì‹¤ ì—´êµí™˜ê¸° ì˜¨ë„ ì´ìƒ í™•ì¸ë¨.\n\n"
    
    "Generate sentences that reflect real-world maintenance log variations with these error patterns."
)

def call_openai_api(messages, max_retries=3, delay=1):
    """OpenAI APIë¥¼ í˜¸ì¶œí•˜ê³  ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤."""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.7,
                max_tokens=2048
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.warning(f"API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {e}")
            if attempt < max_retries - 1:
                time.sleep(delay * (2 ** attempt))  # ì§€ìˆ˜ ë°±ì˜¤í”„
            else:
                logger.error(f"API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                raise

def process_batch(batch, batch_num, total_batches):
    """ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    logger.info(f"ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ë¬¸ì¥)")
    
    prompt_lines = []
    for i, (loc, equip, phen, prio) in enumerate(batch, 1):
        prompt_lines.append(f"{i}. ìœ„ì¹˜: {loc}, ì„¤ë¹„ìœ í˜•: {equip}, í˜„ìƒì½”ë“œ: {phen}, ìš°ì„ ìˆœìœ„: {prio}")
    prompt = "\n".join(prompt_lines)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Create exactly {len(batch)} Korean maintenance report sentences with various error patterns. "
                                   f"Each sentence should be on a new line without numbers, maintaining input order. "
                                   f"Vary the error types across sentences (mixing, spacing, typos, punctuation):\n\n{prompt}"}
    ]

    try:
        output_text = call_openai_api(messages)
        lines = [line.strip() for line in output_text.splitlines() if line.strip()]
        
        # ë²ˆí˜¸ ì œê±°
        cleaned_lines = []
        for line in lines:
            # ë‹¤ì–‘í•œ ë²ˆí˜¸ íŒ¨í„´ ì œê±° (1., 1), 1-, 1. ë“±)
            cleaned_line = re.sub(r'^[\d]+[\.\)\-]\s*', '', line)
            if cleaned_line:
                cleaned_lines.append(cleaned_line)
        
        # ê²°ê³¼ ìˆ˜ê°€ ì…ë ¥ê³¼ ë‹¤ë¥¸ ê²½ìš° ì²˜ë¦¬
        if len(cleaned_lines) != len(batch):
            logger.warning(f"ë°°ì¹˜ {batch_num}: ì˜ˆìƒ {len(batch)}ê°œ, ì‹¤ì œ {len(cleaned_lines)}ê°œ ë¬¸ì¥ ìƒì„±ë¨")
            
            # ë¶€ì¡±í•œ ê²½ìš°: ë§ˆì§€ë§‰ ë¬¸ì¥ ë³µì œ
            while len(cleaned_lines) < len(batch):
                if cleaned_lines:
                    cleaned_lines.append(cleaned_lines[-1] + " (ë³µì œ)")
                else:
                    cleaned_lines.append("ê¸°ë³¸ ë¬¸ì¥")
            
            # ì´ˆê³¼í•œ ê²½ìš°: ë’¤ì˜ ê²ƒë“¤ ì œê±°
            cleaned_lines = cleaned_lines[:len(batch)]
        
        batch_results = []
        for combo, sentence in zip(batch, cleaned_lines):
            loc, equip, phen, prio = combo
            batch_results.append({
                "instruction": "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì„¤ë¹„ í•­ëª©ì„ ì¶”ì¶œí•˜ë¼.",
                "input": sentence,
                "output": {
                    "ìœ„ì¹˜": loc,
                    "ì„¤ë¹„ìœ í˜•": equip,
                    "í˜„ìƒì½”ë“œ": phen,
                    "ìš°ì„ ìˆœìœ„": prio
                }
            })
        
        logger.info(f"ë°°ì¹˜ {batch_num} ì™„ë£Œ: {len(batch_results)}ê°œ ë¬¸ì¥ ìƒì„±")
        return batch_results
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []

# âœ… ë°°ì¹˜ ì²˜ë¦¬ ê°œì„ 
BATCH_SIZE = 50  # ë³¸ê²© ì‹¤í–‰ìš©
batches = [selected_combos[i:i+BATCH_SIZE] for i in range(0, len(selected_combos), BATCH_SIZE)]
total_batches = len(batches)

logger.info(f"ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ë‹¹ {BATCH_SIZE}ê°œ)")

results = []
failed_batches = []

# ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬
for i, batch in enumerate(tqdm(batches, desc="ë°°ì¹˜ ì²˜ë¦¬ ì¤‘"), 1):
    batch_results = process_batch(batch, i, total_batches)
    if batch_results:
        results.extend(batch_results)
    else:
        failed_batches.append(i)
        logger.warning(f"ë°°ì¹˜ {i} ì‹¤íŒ¨ - ì¬ì‹œë„ ëª©ë¡ì— ì¶”ê°€")

# ì‹¤íŒ¨í•œ ë°°ì¹˜ ì¬ì‹œë„
if failed_batches:
    logger.info(f"ì‹¤íŒ¨í•œ {len(failed_batches)}ê°œ ë°°ì¹˜ ì¬ì‹œë„ ì¤‘...")
    for batch_num in failed_batches:
        batch = batches[batch_num - 1]
        logger.info(f"ë°°ì¹˜ {batch_num} ì¬ì‹œë„ ì¤‘...")
        batch_results = process_batch(batch, batch_num, total_batches)
        if batch_results:
            results.extend(batch_results)
        else:
            logger.error(f"ë°°ì¹˜ {batch_num} ì¬ì‹œë„ ì‹¤íŒ¨")

# âœ… ê²°ê³¼ ì €ì¥ (JSONLê³¼ CSV ëª¨ë‘ ì§€ì›)
def save_results(results, base_filename="generated_dataset"):
    """ê²°ê³¼ë¥¼ JSONLê³¼ CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not results:
        logger.error("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # JSONL ì €ì¥
    jsonl_path = f"{base_filename}.jsonl"
    try:
        with open(jsonl_path, "w", encoding="utf-8") as f:
            for item in results:
                json.dump(item, f, ensure_ascii=False)
                f.write("\n")
        logger.info(f"JSONL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {jsonl_path}")
    except Exception as e:
        logger.error(f"JSONL ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # CSV ì €ì¥ (ë¶„ì„ ë° ê²€í† ìš©)
    csv_path = f"{base_filename}.csv"
    try:
        csv_data = []
        for item in results:
            csv_row = {
                "instruction": item["instruction"],
                "input": item["input"],
                "ìœ„ì¹˜": item["output"]["ìœ„ì¹˜"],
                "ì„¤ë¹„ìœ í˜•": item["output"]["ì„¤ë¹„ìœ í˜•"],
                "í˜„ìƒì½”ë“œ": item["output"]["í˜„ìƒì½”ë“œ"],
                "ìš°ì„ ìˆœìœ„": item["output"]["ìš°ì„ ìˆœìœ„"]
            }
            csv_data.append(csv_row)
        
        df = pd.DataFrame(csv_data)
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
    except Exception as e:
        logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return jsonl_path, csv_path

# ê²°ê³¼ ì €ì¥
if results:
    # ì¶œë ¥ í´ë” ìƒì„± (í•„ìš”ì‹œ)
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    # ê²°ê³¼ ì €ì¥ (output í´ë”ì— ì €ì¥)
    jsonl_file, csv_file = save_results(results, f"{output_dir}/generated_dataset")
    
    # ìµœì¢… í†µê³„
    logger.info("="*50)
    logger.info("ğŸ“Š ìµœì¢… ìƒì„± ê²°ê³¼")
    logger.info("="*50)
    logger.info(f"ëª©í‘œ ë¬¸ì¥ ìˆ˜: {target_num:,}")
    logger.info(f"ì‹¤ì œ ìƒì„± ìˆ˜: {len(results):,}")
    logger.info(f"ì„±ê³µë¥ : {len(results)/target_num*100:.1f}%")
    logger.info(f"ì´ ë°°ì¹˜ ìˆ˜: {total_batches}")
    logger.info(f"ì‹¤íŒ¨ ë°°ì¹˜ ìˆ˜: {len(failed_batches)}")
    logger.info("="*50)
    logger.info(f"âœ… JSONL íŒŒì¼: {jsonl_file}")
    logger.info(f"âœ… CSV íŒŒì¼: {csv_file}")
    logger.info("="*50)
    
    # ìƒ˜í”Œ ê²°ê³¼ í‘œì‹œ
    if len(results) >= 3:
        logger.info("ğŸ“ ìƒì„±ëœ ë¬¸ì¥ ìƒ˜í”Œ:")
        for i, sample in enumerate(results[:3], 1):
            logger.info(f"{i}. {sample['input']}")
            logger.info(f"   â†’ ìœ„ì¹˜: {sample['output']['ìœ„ì¹˜']}, "
                       f"ì„¤ë¹„: {sample['output']['ì„¤ë¹„ìœ í˜•']}, "
                       f"í˜„ìƒ: {sample['output']['í˜„ìƒì½”ë“œ']}, "
                       f"ìš°ì„ ìˆœìœ„: {sample['output']['ìš°ì„ ìˆœìœ„']}")
    
else:
    logger.error("âŒ ìƒì„±ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

logger.info("í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì™„ë£Œ")
