# RunPod μ—…λ΅λ“ μ²΄ν¬λ¦¬μ¤νΈ

## π“¦ μ—…λ΅λ“ν•  νμΌλ“¤

### ν•„μ νμΌ β…
- [ ] `mistral_finetuning/` μ „μ²΄ ν΄λ”
- [ ] `generated_dataset.jsonl` (3.12MB)

### μ—…λ΅λ“ ν™•μΈ β…
```bash
# RunPodμ—μ„ ν™•μΈ
ls -la
# λ‹¤μ νμΌλ“¤μ΄ μμ–΄μ•Ό ν•¨:
# - mistral_finetuning/
# - generated_dataset.jsonl
```

## π€ μ‹¤ν–‰ μμ„

### 1λ‹¨κ³„: ν™κ²½ μ„¤μ •
```bash
cd mistral_finetuning
python runpod_setup.py
```

### 2λ‹¨κ³„: νμΈνλ‹ (2κ°€μ§€ λ°©λ²•)

**λ°©λ²• A: Jupyter λ…ΈνΈλ¶ (κ¶μ¥)**
```bash
jupyter notebook finetuning_notebook.ipynb
```

**λ°©λ²• B: Python μ¤ν¬λ¦½νΈ**
```bash
python run_training.py
```

## β±οΈ μμƒ μ†μ” μ‹κ°„
- **L40**: 2-3μ‹κ°„
- **RTX 4090**: 3-4μ‹κ°„

## π” λ¨λ‹ν„°λ§
```bash
# GPU μ‚¬μ©λ‰ ν™•μΈ
nvidia-smi

# λ΅κ·Έ ν™•μΈ
tail -f logs/trainer_state.json

# μ²΄ν¬ν¬μΈνΈ ν™•μΈ
ls -la checkpoints/
```

## π’Ύ κ²°κ³Ό λ‹¤μ΄λ΅λ“
```bash
# λ¨λΈ μ••μ¶•
tar -czf mistral_finetuned.tar.gz checkpoints/

# λλ”
zip -r mistral_finetuned.zip checkpoints/
```

## π† λ¬Έμ  ν•΄κ²°

### CUDA λ©”λ¨λ¦¬ λ¶€μ΅±
```python
# config.pyμ—μ„ λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
per_device_train_batch_size = 2
gradient_accumulation_steps = 8
```

### μ—°κ²° λκΉ€ λ€λΉ„
```bash
# tmux μ‚¬μ©
tmux new -s training
python run_training.py
# Ctrl+B, Dλ΅ μ„Έμ… λ‚κ°€κΈ°

# μ¬μ—°κ²°
tmux attach -t training
```

## β… μ™„λ£ μ²΄ν¬λ¦¬μ¤νΈ
- [ ] ν™κ²½ μ„¤μ • μ™„λ£
- [ ] λ°μ΄ν„° μ „μ²λ¦¬ μ™„λ£
- [ ] νμΈνλ‹ μ‹μ‘
- [ ] μ²΄ν¬ν¬μΈνΈ μ €μ¥ ν™•μΈ
- [ ] μµμΆ… λ¨λΈ μ €μ¥
- [ ] κ²°κ³Ό λ‹¤μ΄λ΅λ“