{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Mistral-7B 파인튜닝: 유지보수 요청 파싱\n",
    "\n",
    "이 노트북은 Mistral-7B-Instruct-v0.3 모델을 사용하여 유지보수 요청 문장을 파싱하는 모델을 파인튜닝하는 과정을 단계별로 안내합니다.\n",
    "\n",
    "## 목표\n",
    "- 자연어로 작성된 작업 요청 문장을 입력받아\n",
    "- 위치, 설비유형, 현상코드, 우선순위 정보를 정확히 식별하여\n",
    "- key:value 형태로 반환하는 모델 학습\n",
    "\n",
    "## 사용 기술\n",
    "- **QLoRA**: 4비트 양자화 + LoRA를 통한 효율적인 파인튜닝\n",
    "- **LoRA**: Low-Rank Adaptation으로 메모리 효율성 확보\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9be5c20b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치\n",
    "\n",
    "**RunPod 환경에서는 이미 대부분의 라이브러리가 설치되어 있습니다.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunPod 환경에서 추가 설치가 필요한 라이브러리만 설치\n",
    "%pip install -q peft bitsandbytes trl wandb datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c561875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트 및 환경 확인\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"작업 디렉토리: {os.getcwd()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
