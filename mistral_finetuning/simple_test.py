#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json

def load_model(model_path="checkpoints/best"):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
        tokenizer.pad_token = tokenizer.eos_token
        
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            "mistralai/Mistral-7B-v0.1",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        model = PeftModel.from_pretrained(base_model, model_path)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def generate_response(model, tokenizer, prompt, max_length=512):
    """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        
        # GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ì œê±°
        response = response[len(prompt):].strip()
        
        return response
        
    except Exception as e:
        print(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def test_maintenance_parsing(model, tokenizer):
    """ìœ ì§€ë³´ìˆ˜ ë°ì´í„° íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    
    test_cases = [
        {
            "input": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. CPU ì‚¬ìš©ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "expected": "fault_type: system_error, severity: high, component: cpu"
        },
        {
            "input": "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. íŒ¨í‚· ì†ì‹¤ë¥ ì´ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "expected": "fault_type: network_issue, severity: medium, component: network"
        },
        {
            "input": "ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ê°€ 10% ë¯¸ë§Œì…ë‹ˆë‹¤.",
            "expected": "fault_type: memory_warning, severity: medium, component: memory"
        }
    ]
    
    print("ğŸ§ª ìœ ì§€ë³´ìˆ˜ ë°ì´í„° íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}:")
        print(f"ì…ë ¥: {test_case['input']}")
        print(f"ì˜ˆìƒ: {test_case['expected']}")
        
        # ì‘ë‹µ ìƒì„±
        response = generate_response(model, tokenizer, test_case['input'])
        
        if response:
            print(f"ì‹¤ì œ: {response}")
            
            # ê°„ë‹¨í•œ í‰ê°€
            if "fault_type" in response.lower():
                print("âœ… fault_type ê°ì§€ë¨")
            else:
                print("âŒ fault_type ê°ì§€ë˜ì§€ ì•ŠìŒ")
        else:
            print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
        
        print("-" * 30)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Mistral-7B íŒŒì¸íŠœë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        print("âŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë¨¼ì € íŒŒì¸íŠœë‹ì„ ì™„ë£Œí•˜ì„¸ìš”: python train.py")
        return
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_maintenance_parsing(model, tokenizer)
    
    # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
    print("\nğŸ’¬ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nì‚¬ìš©ì: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                break
            
            if not user_input:
                continue
            
            response = generate_response(model, tokenizer, user_input)
            
            if response:
                print(f"ëª¨ë¸: {response}")
            else:
                print("ëª¨ë¸: ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()