# RunPodμ—μ„ Mistral-7B νμΈνλ‹ μ™„μ „ κ°€μ΄λ“

μ΄ κ°€μ΄λ“λ” RunPod ν΄λΌμ°λ“ GPU ν”λ«νΌμ—μ„ Mistral-7B λ¨λΈμ„ νμΈνλ‹ν•λ” λ°©λ²•μ„ λ‹¨κ³„λ³„λ΅ μ„¤λ…ν•©λ‹λ‹¤.

## π“‹ μ‚¬μ „ μ¤€λΉ„

### 1.1 RunPod κ³„μ • λ° GPU Pod
1. [RunPod](https://runpod.io)μ— λ΅κ·ΈμΈ
2. "GPU Pods" μ„Ήμ…μΌλ΅ μ΄λ™
3. **κ¶μ¥ μ‚¬μ–‘**:
   - **GPU**: RTX L40 (48GB VRAM) λλ” RTX 4090 (24GB VRAM)
   - **ν…ν”λ¦Ώ**: PyTorch λλ” Jupyter Notebook
   - **μ¤ν† λ¦¬μ§€**: μµμ† 50GB
   - **RAM**: μµμ† 32GB

### 1.2 ν”„λ΅μ νΈ νμΌ μ¤€λΉ„
λ‹¤μ νμΌλ“¤μ„ μ¤€λΉ„ν•μ—¬ μ—…λ΅λ“:
```
mistral_finetuning/
β”β”€β”€ train.py                    # νμΈνλ‹ λ©”μΈ μ¤ν¬λ¦½νΈ
β”β”€β”€ config.py                   # μ„¤μ • νμΌ (λ©”λ¨λ¦¬ μµμ ν™” ν¬ν•¨)
β”β”€β”€ data_preprocessing.py       # λ°μ΄ν„° μ „μ²λ¦¬
β”β”€β”€ evaluate.py                 # ν‰κ°€ μ¤ν¬λ¦½νΈ
β”β”€β”€ inference.py                # μ¶”λ΅  μ¤ν¬λ¦½νΈ
β”β”€β”€ runpod_setup.py            # RunPod μ„¤μ • μ¤ν¬λ¦½νΈ
β”β”€β”€ requirements.txt            # μμ΅΄μ„± λ©λ΅
β”β”€β”€ finetuning_notebook.ipynb   # Jupyter λ…ΈνΈλ¶
β””β”€β”€ generated_dataset.jsonl     # ν•™μµ λ°μ΄ν„° (μ¤‘μ”!)
```

## π€ 1λ‹¨κ³„: RunPod Pod μƒμ„± λ° μ ‘μ†

### 1.1 Pod μƒμ„±
1. RunPod λ€μ‹λ³΄λ“μ—μ„ "GPU Pods" ν΄λ¦­
2. **κ¶μ¥ μ„¤μ •**:
   - GPU: RTX L40 (48GB VRAM)
   - ν…ν”λ¦Ώ: PyTorch 2.0
   - μ¤ν† λ¦¬μ§€: 50GB
3. "Deploy" ν΄λ¦­ν•μ—¬ Pod μƒμ„±

### 1.2 Pod μ ‘μ†
1. Podκ°€ "Running" μƒνƒκ°€ λλ©΄ "Connect" ν΄λ¦­
2. **ν„°λ―Έλ„ μ ‘μ†** (κ¶μ¥):
   - "Terminal" νƒ­ μ„ νƒ
   - SSH λλ” μ›Ή ν„°λ―Έλ„ μ‚¬μ©

## π“ 2λ‹¨κ³„: ν”„λ΅μ νΈ νμΌ μ—…λ΅λ“

### 2.1 νμΌ μ—…λ΅λ“ λ°©λ²•
**λ°©λ²• 1: Jupyter Lab νμΌ λΈλΌμ°μ €**
1. "Connect" β†’ "Jupyter Lab" μ„ νƒ
2. νμΌ λΈλΌμ°μ €μ—μ„ `mistral_finetuning/` ν΄λ” μ—…λ΅λ“

**λ°©λ²• 2: Git ν΄λ΅ **
```bash
git clone <your-repository-url>
cd mistral_finetuning
```

**λ°©λ²• 3: μ§μ ‘ μ—…λ΅λ“**
```bash
# λ΅μ»¬μ—μ„ νμΌ μ••μ¶•
tar -czf mistral_finetuning.tar.gz mistral_finetuning/

# RunPodμ—μ„ λ‹¤μ΄λ΅λ“
wget <your-file-url>
tar -xzf mistral_finetuning.tar.gz
```

### 2.2 ν•„μ νμΌ ν™•μΈ
```bash
cd mistral_finetuning
ls -la

# λ‹¤μ νμΌλ“¤μ΄ μμ–΄μ•Ό ν•¨:
# - train.py
# - config.py
# - runpod_setup.py
# - generated_dataset.jsonl
```

## β™οΈ 3λ‹¨κ³„: ν™κ²½ μ„¤μ •

### 3.1 μλ™ μ„¤μ • (κ¶μ¥)
```bash
cd mistral_finetuning
python runpod_setup.py
```

μ΄ μ¤ν¬λ¦½νΈλ” μλ™μΌλ΅:
- β… μ‹μ¤ν… μμ΅΄μ„± μ„¤μΉ (tmux λ“±)
- β… GPU ν™κ²½ ν™•μΈ
- β… λ¨λ“  ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ
- β… Hugging Face μΈμ¦ μ„¤μ •
- β… λ””λ ‰ν† λ¦¬ μƒμ„±
- β… λ°μ΄ν„° μ „μ²λ¦¬ μ‹¤ν–‰

### 3.2 μλ™ μ„¤μ • (ν•„μ”μ‹)
```bash
# μ‹μ¤ν… μμ΅΄μ„±
apt update && apt install -y tmux

# Python λΌμ΄λΈλ¬λ¦¬
pip install -r requirements.txt

# ν™κ²½ λ³€μ μ„¤μ •
export TOKENIZERS_PARALLELISM=false
```

## π” 4λ‹¨κ³„: Hugging Face μΈμ¦ (ν•„μ”μ‹)

### 4.1 ν† ν° κΈ°λ° μΈμ¦
```bash
# Hugging Face ν† ν°μΌλ΅ λ΅κ·ΈμΈ
huggingface-cli login --token YOUR_TOKEN

# λλ” λ€ν™”ν• λ΅κ·ΈμΈ
huggingface-cli login
```

### 4.2 ν™κ²½ λ³€μ μ„¤μ •
```bash
export HUGGING_FACE_HUB_TOKEN=your_token_here
```

### 4.3 κ³µκ° λ¨λΈ μ‚¬μ© (κ¶μ¥)
ν„μ¬ μ„¤μ •μ€ κ³µκ° λ¨λΈ `mistralai/Mistral-7B-v0.1`μ„ μ‚¬μ©ν•λ―€λ΅ μΈμ¦μ΄ ν•„μ”ν•μ§€ μ•μµλ‹λ‹¤.

## π― 5λ‹¨κ³„: νμΈνλ‹ μ‹¤ν–‰

### 5.1 tmux μ„Έμ…μ—μ„ μ‹¤ν–‰ (κ¶μ¥)
```bash
# tmux μ„Έμ… μ‹μ‘
tmux new -s finetuning

# νμΈνλ‹ μ‹¤ν–‰
python train.py

# μ„Έμ…μ—μ„ λ‚κ°€κΈ° (ν•™μµμ€ κ³„μ†λ¨)
# Ctrl+B, D
```

### 5.2 λ°±κ·ΈλΌμ΄λ“ μ‹¤ν–‰
```bash
# λ°±κ·ΈλΌμ΄λ“μ—μ„ μ‹¤ν–‰
nohup python train.py > training.log 2>&1 &

# ν”„λ΅μ„Έμ¤ ν™•μΈ
ps aux | grep python

# λ΅κ·Έ μ‹¤μ‹κ°„ ν™•μΈ
tail -f training.log
```

### 5.3 Jupyter λ…ΈνΈλ¶ μ‹¤ν–‰ (μ„ νƒμ‚¬ν•­)
```bash
# Jupyter Lab μ‹¤ν–‰
jupyter lab --allow-root --no-browser --port=8888

# λλ” Jupyter Notebook
jupyter notebook --allow-root --no-browser --port=8888
```

## π“ 6λ‹¨κ³„: ν•™μµ λ¨λ‹ν„°λ§

### 6.1 μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§
```bash
# GPU μ‚¬μ©λ‰ ν™•μΈ
watch -n 1 nvidia-smi

# ν”„λ΅μ„Έμ¤ ν™•μΈ
ps aux | grep python

# μ²΄ν¬ν¬μΈνΈ ν™•μΈ
watch -n 10 ls -la checkpoints/

# λ΅κ·Έ ν™•μΈ
tail -f training.log
```

### 6.2 tmux μ„Έμ… κ΄€λ¦¬
```bash
# μ„Έμ… λ©λ΅ ν™•μΈ
tmux list-sessions

# μ„Έμ… μ¬μ—°κ²°
tmux attach -t finetuning

# μƒ μ„Έμ…μ—μ„ λ¨λ‹ν„°λ§
tmux new -s monitoring
```

### 6.3 ν•™μµ μ§„ν–‰ μƒν™© ν™•μΈ
```bash
# ν•™μµ λ΅κ·Έ ν™•μΈ
tail -f logs/trainer_state.json

# μ²΄ν¬ν¬μΈνΈ ν¬κΈ° ν™•μΈ
du -sh checkpoints/*

# GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

## π”„ 7λ‹¨κ³„: ν•™μµ μ¤‘λ‹¨ λ° μ¬κ°

### 7.1 μ•μ „ν• μ¤‘λ‹¨
```bash
# tmux μ„Έμ…μ—μ„ Ctrl+C
# λλ” ν”„λ΅μ„Έμ¤ μΆ…λ£
pkill -f "python train.py"
```

### 7.2 μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬κ°
```bash
# κ°€μ¥ μµκ·Ό μ²΄ν¬ν¬μΈνΈ ν™•μΈ
ls -la checkpoints/

# μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬μ‹μ‘
python train.py --resume checkpoints/checkpoint-1000
```

### 7.3 μ—°κ²° λκΉ€ λ€λΉ„
```bash
# tmux μ„Έμ… μ‚¬μ© (κ¶μ¥)
tmux new -s finetuning
python train.py

# λλ” nohup μ‚¬μ©
nohup python train.py > training.log 2>&1 &
```

## π“ 8λ‹¨κ³„: ν•™μµ μ™„λ£ λ° κ²°κ³Ό ν™•μΈ

### 8.1 ν•™μµ μ™„λ£ ν™•μΈ
```bash
# μµμΆ… μ²΄ν¬ν¬μΈνΈ ν™•μΈ
ls -la checkpoints/

# ν•™μµ λ΅κ·Έ ν™•μΈ
tail -20 training.log

# GPU μ‚¬μ©λ‰ ν™•μΈ (0%λ΅ λ–¨μ–΄μ§)
nvidia-smi
```

### 8.2 λ¨λΈ νμΌ μ••μ¶•
```bash
# μ²΄ν¬ν¬μΈνΈ μ••μ¶•
tar -czf mistral_finetuned_model.tar.gz checkpoints/

# λλ” ZIP ν•νƒ
zip -r mistral_finetuned_model.zip checkpoints/
```

### 8.3 κ²°κ³Ό λ‹¤μ΄λ΅λ“
1. Jupyter Lab νμΌ λΈλΌμ°μ €μ—μ„ μ••μ¶• νμΌ μ„ νƒ
2. μ°ν΄λ¦­ β†’ "Download" μ„ νƒ
3. λ΅μ»¬ μ»΄ν“¨ν„°μ— μ €μ¥

## π§ 9λ‹¨κ³„: λ¨λΈ ν…μ¤νΈ

### 9.1 κ°„λ‹¨ν• μ¶”λ΅  ν…μ¤νΈ
```bash
# μ¶”λ΅  μ¤ν¬λ¦½νΈ μ‹¤ν–‰
python inference.py --model_path checkpoints/best

# λλ” κ°„λ‹¨ν• ν…μ¤νΈ
python simple_test.py
```

### 9.2 μ„±λ¥ ν‰κ°€
```bash
# ν‰κ°€ μ¤ν¬λ¦½νΈ μ‹¤ν–‰
python evaluate.py --model_path checkpoints/best
```

## π’° 10λ‹¨κ³„: λΉ„μ© μµμ ν™”

### 10.1 Pod κ΄€λ¦¬
- **ν•™μµ μ™„λ£ ν›„ μ¦‰μ‹ Pod μΆ…λ£**
- **ν•„μ”μ‹ μ¤λƒ…μƒ· μ €μ¥**ν•μ—¬ μ¬μ‚¬μ©
- **μ¥μ‹κ°„ μ‚¬μ©ν•μ§€ μ•μ„ λ•λ” "Pause"** μ‚¬μ©

### 10.2 μ„¤μ • μµμ ν™”
```python
# λ©”λ¨λ¦¬ μ μ•½ μ„¤μ • (config.pyμ—μ„)
per_device_train_batch_size = 2  # λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
gradient_accumulation_steps = 8  # κ·Έλλ””μ–ΈνΈ λ„μ  μ¦κ°€
dataloader_num_workers = 0       # μ›μ»¤ μ μ¤„μ΄κΈ°
```

## π› οΈ 11λ‹¨κ³„: λ¬Έμ  ν•΄κ²°

### 11.1 μΌλ°μ μΈ λ¬Έμ λ“¤

**CUDA λ©”λ¨λ¦¬ λ¶€μ΅±**
```bash
# ν™κ²½ λ³€μ μ„¤μ •
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false

# λ©”λ¨λ¦¬ μ •λ¦¬
python -c "import torch; torch.cuda.empty_cache()"

# λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ° (config.pyμ—μ„)
per_device_train_batch_size = 2
gradient_accumulation_steps = 8
```

**Hugging Face μΈμ¦ μ¤λ¥**
```bash
# κ³µκ° λ¨λΈ μ‚¬μ© (κ¶μ¥)
# config.pyμ—μ„ model_nameμ„ "mistralai/Mistral-7B-v0.1"λ΅ μ„¤μ •

# λλ” ν† ν° μ„¤μ •
export HUGGING_FACE_HUB_TOKEN=your_token
```

**ν† ν¬λ‚μ΄μ € κ²½κ³ **
```bash
# ν™κ²½ λ³€μ μ„¤μ •
export TOKENIZERS_PARALLELISM=false
```

**μ—°κ²° λκΉ€**
```bash
# tmux μ„Έμ… μ‚¬μ©
tmux new -s finetuning
python train.py

# μ„Έμ… μ¬μ—°κ²°
tmux attach -t finetuning
```

### 11.2 μ„±λ¥ ν™•μΈ
```bash
# GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰
nvidia-smi

# ν•™μµ μ†λ„ ν™•μΈ
tail -f training.log | grep "it/s"

# μ²΄ν¬ν¬μΈνΈ μƒμ„± ν™•μΈ
ls -la checkpoints/
```

### 11.3 λ©”λ¨λ¦¬ μµμ ν™” ν
```bash
# λ©”λ¨λ¦¬ λ‹¨νΈν™” λ°©μ§€
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# ν† ν¬λ‚μ΄μ € λ³‘λ ¬ν™” λΉ„ν™μ„±ν™”
export TOKENIZERS_PARALLELISM=false

# λ©”λ¨λ¦¬ μΊμ‹ μ •λ¦¬
python -c "import torch; torch.cuda.empty_cache()"
```

## π“‹ 12λ‹¨κ³„: μ²΄ν¬λ¦¬μ¤νΈ

### 12.1 μ‚¬μ „ μ¤€λΉ„
- [ ] RunPod κ³„μ • μƒμ„±
- [ ] GPU Pod μƒμ„± (L40 λλ” 4090)
- [ ] ν”„λ΅μ νΈ νμΌ μ¤€λΉ„
- [ ] λ°μ΄ν„° νμΌ μ¤€λΉ„

### 12.2 ν™κ²½ μ„¤μ •
- [ ] νμΌ μ—…λ΅λ“ μ™„λ£
- [ ] `python runpod_setup.py` μ‹¤ν–‰
- [ ] GPU ν™κ²½ ν™•μΈ
- [ ] λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ μ™„λ£

### 12.3 νμΈνλ‹ μ‹¤ν–‰
- [ ] tmux μ„Έμ… μ‹μ‘
- [ ] `python train.py` μ‹¤ν–‰
- [ ] GPU μ‚¬μ©λ¥  ν™•μΈ (80-90%)
- [ ] μ²΄ν¬ν¬μΈνΈ μƒμ„± ν™•μΈ

### 12.4 λ¨λ‹ν„°λ§
- [ ] μ‹¤μ‹κ°„ λ΅κ·Έ ν™•μΈ
- [ ] GPU μ‚¬μ©λ‰ λ¨λ‹ν„°λ§
- [ ] μ²΄ν¬ν¬μΈνΈ λ°±μ—…
- [ ] ν•™μµ μ™„λ£ ν™•μΈ

### 12.5 κ²°κ³Ό μ²λ¦¬
- [ ] λ¨λΈ νμΌ μ••μ¶•
- [ ] κ²°κ³Ό λ‹¤μ΄λ΅λ“
- [ ] Pod μΆ…λ£
- [ ] λΉ„μ© ν™•μΈ

## π“ μ§€μ› λ° λ¬Έμ

- **RunPod κ³µμ‹ λ¬Έμ„**: [docs.runpod.io](https://docs.runpod.io)
- **Discord μ»¤λ®¤λ‹ν‹°**: RunPod κ³µμ‹ Discord
- **GitHub Issues**: ν”„λ΅μ νΈ μ €μ¥μ†
- **κΈ°μ μ  λ¬Έμ **: μ΄ κ°€μ΄λ“μ λ¬Έμ  ν•΄κ²° μ„Ήμ… μ°Έμ΅°

## β±οΈ μμƒ μ†μ” μ‹κ°„

- **ν™κ²½ μ„¤μ •**: 10-15λ¶„
- **λ¨λΈ λ‹¤μ΄λ΅λ“**: 5-10λ¶„
- **νμΈνλ‹**: 2-3μ‹κ°„ (L40), 3-4μ‹κ°„ (4090)
- **κ²°κ³Ό μ²λ¦¬**: 5-10λ¶„

## π’΅ ν

1. **μ—°κ²° μ•μ •μ„±**: tmux μ„Έμ… μ‚¬μ©μΌλ΅ μ—°κ²° λκΉ€ λ°©μ§€
2. **λ¨λ‹ν„°λ§**: μƒ ν„°λ―Έλ„μ—μ„ μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§
3. **λ°±μ—…**: μ •κΈ°μ μΌλ΅ μ²΄ν¬ν¬μΈνΈ λ°±μ—…
4. **λΉ„μ©**: ν•™μµ μ™„λ£ ν›„ μ¦‰μ‹ Pod μΆ…λ£
5. **μ„±λ¥**: GPU μ‚¬μ©λ¥  80-90%κ°€ μ •μƒ

## π”§ λ©”λ¨λ¦¬ μµμ ν™” μ„¤μ •

### ν„μ¬ μµμ ν™”λ μ„¤μ •
- **λ°°μΉ ν¬κΈ°**: 2 (λ©”λ¨λ¦¬ μ μ•½)
- **κ·Έλλ””μ–ΈνΈ λ„μ **: 8 (μ‹¤μ  λ°°μΉ ν¬κΈ° 16 μ μ§€)
- **μ‹ν€€μ¤ κΈΈμ΄**: 1024 (μ‹¤μ  λ°μ΄ν„° μµλ€ 39 ν† ν°)
- **μ›μ»¤ μ**: 0 (λ©”λ¨λ¦¬ μ μ•½)
- **κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν…**: ν™μ„±ν™”

### ν™κ²½ λ³€μ μ„¤μ •
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
```

---

**μ΄ κ°€μ΄λ“λ¥Ό λ”°λΌν•λ©΄ RunPodμ—μ„ μ•μ •μ μΌλ΅ Mistral-7B νμΈνλ‹μ„ μ™„λ£ν•  μ μμµλ‹λ‹¤!** π€